Therefore the problem of predicting performance of a conﬁgurable system reduces to the problem of *learning* the performance function with a small number of its values given by the measurements. 

Although there are many established  learning  algorithms [1], [5], [8], [9], [10], [19], it is well known that arbitrary Boolean functions of this form simply cannot be learned. Since **the domain of such functions is a ﬁnite set**,  with an arbitrary function *f* , knowing *f* (*a*) provides practically no information about *f* (*b*) unless *a* = *b*. ( 自变量不同, 就没什么关系 ) That is to say, **performance values cannot be predicted by merely taking samples of conﬁgurations.** ( 也是难点所在? )

Fortunately, previous empirical results [6], [13] have shown that **performance functions** of actual software systems are not arbitrary, **but rather structured**, hence can be potentially learned **effectively**.





**actual performance functions of being close to Fourier sparse functions, namely functions that have many 0’s in their Fourier decompositions [11].**

we have proposed an algorithm that is able **learn these Fourier sparse functions** to user speciﬁed accuracy and conﬁdence level, by **taking only a small number of sample conﬁgurations**, such that we in turn achieve accurate predictions of the actual underlying performance function of a conﬁgurable system.



- A theoretical model and formulation of conﬁgurable software performance prediction **in terms of Fourier learning of Boolean functions.**
- An algorithm for learning conﬁgurable software performance functions via their Fourier decomposition.
- An implementation, analysis and evaluation of the algorithm, with its strengths and relative weaknesses **compared to existing performance prediction methods.**



**learning a function f is equivalent to learning all of its Fourier coefficients.**



In particular, as we will discuss in Section III, functions constructed from real software systems are **typically close to Fourier sparse**, in which case, predicting its performances becomes equivalent to **estimating only the large Fourier coefficients of its corresponding function f.** This is exactly what our algorithm does.





If $dist(f, h) ≥ 4d/3$, our estimate will be larger than $d$. Therefore we are confident that the original function f is not  sufficiently close to any t-sparse function. In this case, **we need  to increase t and draw more samples** to guarantee $dist(f, h) ≤ γ$.



**归一化:** performance values of a particular system, e.g. between 0 and  max. Then f **can be normalized by subtracting max/2 from  each f(x) and then dividing the result by max/2.**



![1551586014409](C:\Users\Kai\AppData\Roaming\Typora\typora-user-images\1551586014409.png)

前面有证过, 正交标准基提取的效应





**Step 1:** Given a function f, let g be the function obtained from f by **only keeping its t largest Fourier coefficients** and set the rest to 0, **such that dist(f, g) ≤ d.** Then by construction, **g is t-sparse.**

**Step 2:** Let **h1** be the function obtained from g **by replacing all its Fourier coefficients smaller than $d/t$ by 0**, namely:

$h_{1}(x)=\sum_{z : \hat{g}(z) \geq d / t} \hat{g}(z) \chi_{z}(x)$

现在构造完了 $ g, h1​$

那篇论文看了一半看到学习算法那里, 大致懂了是怎么做的, 处理傅里叶系数, 根据标准正交基性质把它表成$\frac{1}{|S|} \sum_{x \in S} f(x) \chi_{z}(x)$, 然后由此开始估计 h ( Step1, 2...5 ), 根据Hoeffding不等式推得训练集个数$m=\frac{2}{\epsilon^{2}}((\log 2)(n+1)+\log (1 / \delta))$, 定义$t-sparse$函数g( dist(f, g) <= d ), 再控制系数大小关系得到h1, h2 函数, $\epsilon$ 用h1, h2的范数替换, 得到一个可计算的训练集个数, 在通过不等式证明( Step 4 ) 说明这可以让dist(h, f) <= 4d/3.

感觉就是在搞数学, 这方面我可能还挺擅长的, 可以做这个东西, 接下来我该做什么? 再用几天时间把这篇论文中一些不理解的点再细抠一下, 比如Step4, Step5中的各种系数的大小控制( 小于一个值直接看为零 ), 为什么后面又用$1-\delta$去估计dist(h, f)了( 就是代入Hoeffding不等式的是$\delta$) ( 之前是$\frac{\delta}{2^{n}}$ ), 还是需要做其他事情?
